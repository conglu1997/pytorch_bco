{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11,) (2,)\n",
      "Ep 1: -41.53\n",
      "Ep 2: -40.14\n",
      "Ep 3: -44.62\n",
      "Ep 4: -40.81\n",
      "Ep 5: -43.25\n",
      "Ep 6: -48.14\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('Reacher-v2')\n",
    "print(env.observation_space.shape, env.action_space.shape)\n",
    "\n",
    "for i in range(6):\n",
    "    env.reset()\n",
    "    rew = 0\n",
    "    \n",
    "    while True:\n",
    "        _, r, done, _ = env.step(env.action_space.sample())\n",
    "        \n",
    "        rew += r\n",
    "        \n",
    "        if done==True:\n",
    "            print('Ep %d: %.2f' % (i+1, rew))\n",
    "            \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCO(nn.Module):\n",
    "    def __init__(self, env, policy='mlp'):\n",
    "        super(BCO, self).__init__()\n",
    "        \n",
    "        self.policy = policy\n",
    "        self.act_n = env.action_space.shape[0]\n",
    "        \n",
    "        if self.policy=='mlp':\n",
    "            self.obs_n = env.observation_space.shape[0]\n",
    "            self.pol = nn.Sequential(*[nn.Linear(self.obs_n, 32), nn.LeakyReLU(), \n",
    "                                       nn.Linear(32, 32), nn.LeakyReLU(), \n",
    "                                       nn.Linear(32, self.act_n)])\n",
    "            self.inv = nn.Sequential(*[nn.Linear(self.obs_n*2, 32), nn.LeakyReLU(), \n",
    "                                       nn.Linear(32, 32), nn.LeakyReLU(),  \n",
    "                                       nn.Linear(32, self.act_n)])\n",
    "        \n",
    "        elif self.policy=='cnn':\n",
    "            pass\n",
    "    \n",
    "    def pred_act(self, obs):\n",
    "        out = self.pol(obs)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def pred_inv(self, obs1, obs2):\n",
    "        obs = T.cat([obs1, obs2], dim=1)\n",
    "        out = self.inv(obs)\n",
    "        \n",
    "        return out\n",
    "\n",
    "POLICY = 'mlp'\n",
    "model = BCO(env, policy=POLICY).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DS_Inv(Dataset):\n",
    "    def __init__(self, trajs):\n",
    "        self.dat = []\n",
    "        \n",
    "        for traj in trajs:\n",
    "            for dat in traj:\n",
    "                obs, act, new_obs = dat\n",
    "                \n",
    "                self.dat.append([obs, new_obs, act])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dat)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        obs, new_obs, act = self.dat[idx]\n",
    "        \n",
    "        return obs, new_obs, act\n",
    "\n",
    "class DS_Policy(Dataset):\n",
    "    def __init__(self, traj):\n",
    "        self.dat = []\n",
    "        \n",
    "        for dat in traj:\n",
    "            obs, act = dat\n",
    "                \n",
    "            self.dat.append([obs, act])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dat)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        obs, act = self.dat[idx]\n",
    "        \n",
    "        return obs, act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "torch.Size([100, 11]) torch.Size([100, 11])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "trajs_demo = pickle.load(open('Demo/demo_reacher.pkl', 'rb'))\n",
    "ld_demo = DataLoader(DS_Inv(trajs_demo), batch_size=100)\n",
    "\n",
    "print(len(ld_demo))\n",
    "for obs1, obs2, _ in ld_demo:\n",
    "    print(obs1.shape, obs2.shape)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss().cuda()\n",
    "optim = T.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "EPOCHS = 20\n",
    "M = 5000\n",
    "\n",
    "EPS = 0.9\n",
    "DECAY = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6baf88aa1f614f8fbfae1d3614e410ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1: reward=-41.75\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695756c3f55944b6813a9a0ce4f2e007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1: loss_inv=0.267\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "682c13aa518d4dca900b335fcb4285db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1: loss_policy=0.007\n",
      "Ep 2: reward=-34.40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b72cc6748294330acfe8b1d9c83db97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=313), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 2: loss_inv=0.083\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfae6c07955a4788bc24fa794585088c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 2: loss_policy=0.004\n",
      "Ep 3: reward=-43.24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91be01ffce754ed4bdfcc368851d6dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=469), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 3: loss_inv=0.052\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc8fcb6a1e34f4197df266c38565219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 3: loss_policy=0.003\n",
      "Ep 4: reward=-20.75\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d9db8f804b44d891d8fd408330665f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=625), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 4: loss_inv=0.032\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f3bcb2494b45fea6a44ba0cf2efac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 4: loss_policy=0.003\n",
      "Ep 5: reward=-20.34\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b6b94127d3425d99b6ad5b33a542c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 5: loss_inv=0.022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a452d93c0be4052a9df2e324ea94b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 5: loss_policy=0.002\n",
      "Ep 6: reward=-16.03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb0d56c5d40c448e9ed11b75ab0c41ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=938), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 6: loss_inv=0.016\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbce773ce2d04a928436b0603814b52c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 6: loss_policy=0.002\n",
      "Ep 7: reward=-16.49\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c461f5c0ae654d2581ab95c9ad42e8bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1094), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 7: loss_inv=0.013\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb80c755e5fc4a3988c75c02c08acfa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 7: loss_policy=0.002\n",
      "Ep 8: reward=-13.95\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762a1ab8cf744c4a8b015e1a1738390c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 8: loss_inv=0.010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd7684b38c02431b9f63fcd636a9424a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 8: loss_policy=0.002\n",
      "Ep 9: reward=-12.96\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf17ec223ba4ace8d4e92f1d5c1caa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1407), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 9: loss_inv=0.009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab9ed9aefeb43d8acf3697b55ac6253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 9: loss_policy=0.002\n",
      "Ep 10: reward=-12.78\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "244c14261a764d1d9f345ff78066be88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1563), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 10: loss_inv=0.008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33549463204d44a4acc2b2c51c316e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 10: loss_policy=0.002\n",
      "Ep 11: reward=-13.51\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce472618a4974321a87549410c105315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1719), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 11: loss_inv=0.007\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a1766361874b0aa0a49e19f3a951fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 11: loss_policy=0.002\n",
      "Ep 12: reward=-12.55\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5730de97c0f42588a3e0d883c9b3efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1875), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 12: loss_inv=0.006\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37c40eb3d5b43d79c3d49438a719b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 12: loss_policy=0.002\n",
      "Ep 13: reward=-11.60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df29e98d69e545d8abd25bb69c22970f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2032), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 13: loss_inv=0.005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813e5d61ad96485a96ce6345881bbe34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 13: loss_policy=0.002\n",
      "Ep 14: reward=-12.66\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a3582ecbf843ce86aa37a774c87466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2188), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 14: loss_inv=0.005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6bd1f6bfda042449f356ed61ce091cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 14: loss_policy=0.003\n",
      "Ep 15: reward=-12.44\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ee9da8e957429e9193c3a66ef6816b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2344), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 15: loss_inv=0.004\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85d107cc6234c2ca78a4405128ba824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 15: loss_policy=0.003\n",
      "Ep 16: reward=-12.20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef998dcfa4ef4854877769925583d0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 16: loss_inv=0.004\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3642a757d3f04dfe858b1dd02106f944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 16: loss_policy=0.003\n",
      "Ep 17: reward=-11.53\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc64bc80fee94ca0b69f6228ca6104f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2657), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 17: loss_inv=0.003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b4a6f3ddc94e5783b9bb93827862b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 17: loss_policy=0.002\n",
      "Ep 18: reward=-12.09\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70b5e1a37f74a3fafdd5d2072db049d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2813), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 18: loss_inv=0.003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f75f902de5419b80d2b6ae90286017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 18: loss_policy=0.002\n",
      "Ep 19: reward=-11.29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe6fa0a401f403da15982c575b54fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2969), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 19: loss_inv=0.003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4f21bf3ddc4003bde675df2375368b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 19: loss_policy=0.002\n",
      "Ep 20: reward=-11.78\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fba77abae4149cc80d80e99ce9d1858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3125), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 20: loss_inv=0.002\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7668098c46f942b79a639d14b192c567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 20: loss_policy=0.002\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trajs_inv = []\n",
    "\n",
    "for e in tqdm(range(EPOCHS)):\n",
    "    \n",
    "    # step1, generate inverse samples\n",
    "    cnt = 0\n",
    "    epn = 0\n",
    "    \n",
    "    rews = 0\n",
    "        \n",
    "    while True:\n",
    "        traj = []\n",
    "        rew = 0\n",
    "            \n",
    "        obs = env.reset()\n",
    "        while True:\n",
    "            inp = T.from_numpy(obs).view(((1, )+obs.shape)).float().cuda()\n",
    "            out = model.pred_act(inp).cpu().detach().numpy()\n",
    "                \n",
    "            if np.random.rand()>=EPS:\n",
    "                act = out[0]\n",
    "            else:\n",
    "                act = env.action_space.sample()\n",
    "                \n",
    "            new_obs, r, done, _ = env.step(act)\n",
    "                \n",
    "            traj.append([obs, act, new_obs])\n",
    "            obs = new_obs\n",
    "            rew += r\n",
    "            \n",
    "            cnt += 1\n",
    "                \n",
    "            if done==True:\n",
    "                rews += rew\n",
    "                trajs_inv.append(traj)\n",
    "                \n",
    "                epn += 1\n",
    "                \n",
    "                break\n",
    "        \n",
    "        if cnt >= M:\n",
    "            break\n",
    "        \n",
    "    rews /= epn\n",
    "    print('Ep %d: reward=%.2f' % (e+1, rews))\n",
    "        \n",
    "    # step2, update inverse model\n",
    "    ld_inv = DataLoader(DS_Inv(trajs_inv), batch_size=32, shuffle=True)\n",
    "    \n",
    "    with tqdm(ld_inv) as TQ:\n",
    "        ls_ep = 0\n",
    "        \n",
    "        for obs1, obs2, act in TQ:\n",
    "            out = model.pred_inv(obs1.float().cuda(), obs2.float().cuda())\n",
    "            ls_bh = loss_func(out, act.cuda())\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            ls_bh.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            ls_bh = ls_bh.cpu().detach().numpy()\n",
    "            TQ.set_postfix(loss_inv='%.3f' % (ls_bh))\n",
    "            ls_ep += ls_bh\n",
    "        \n",
    "        ls_ep /= len(TQ)\n",
    "        print('Ep %d: loss_inv=%.3f' % (e+1, ls_ep))\n",
    "    \n",
    "    # step3, predict inverse action for demo samples\n",
    "    traj_policy = []\n",
    "    \n",
    "    for obs1, obs2, _ in ld_demo:\n",
    "        out = model.pred_inv(obs1.float().cuda(), obs2.float().cuda())\n",
    "        \n",
    "        obs = obs1.cpu().detach().numpy()\n",
    "        out = out.cpu().detach().numpy()\n",
    "        \n",
    "        for i in range(100):\n",
    "            traj_policy.append([obs[i], out[i]])\n",
    "    \n",
    "    # step4, update policy via demo samples\n",
    "    ld_policy = DataLoader(DS_Policy(traj_policy), batch_size=32, shuffle=True)\n",
    "    \n",
    "    with tqdm(ld_policy) as TQ:\n",
    "        ls_ep = 0\n",
    "        \n",
    "        for obs, act in TQ:\n",
    "            out = model.pred_act(obs.float().cuda())\n",
    "            ls_bh = loss_func(out, act.cuda())\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            ls_bh.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            ls_bh = ls_bh.cpu().detach().numpy()\n",
    "            TQ.set_postfix(loss_policy='%.3f' % (ls_bh))\n",
    "            ls_ep += ls_bh\n",
    "        \n",
    "        ls_ep /= len(TQ)\n",
    "        print('Ep %d: loss_policy=%.3f' % (e+1, ls_ep))\n",
    "    \n",
    "    # step5, save model\n",
    "    T.save(model.state_dict(), 'Model/model_reacher_%d.pt' % (e+1))\n",
    "    \n",
    "    EPS *= DECAY"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
